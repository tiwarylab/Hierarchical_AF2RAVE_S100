[Model Parameters]
# Dimension of RC or bottleneck
dt = [Time]
d = [2]
# Encoder type (Linear or Nonlinear)
encoder_type = Linear
# Number of nodes in each hidden layer of the encoder
neuron_num1 = [32]
# Number of nodes in each hidden layer of the decoder
neuron_num2 = [128]
[Training Parameters]
batch_size = 128
# Threshold in terms of the change of the predicted state population for measuring the convergence of training
threshold = 0.01
# Number of epochs with the change of the state population smaller than the threshold after which this iteration of training finishes
patience = 2
# Number of refinements
refinements = 8
# Period of learning rate decay
lr_scheduler_step_size = 5
# Multiplicative factor of learning rate decay. Default: 1 (No learning rate decay)
lr_scheduler_gamma = 0.8 
# By default, we save the model every 10000 steps
log_interval = 10000
# Initial learning rate of Adam optimizer
learning_rate = [1e-4]
# Hyper-parameter beta
beta = [5e-2]

[Other Controls]
# Random seed
seed = [0]
# Whether to refine the labels during the training process
UpdateLabel = True
# Whether save trajectory results
SaveTrajResults = True


[Data]


 traj_data = [/scratch/zt1/project/tiwary-prj/user/xg23/S100/rave/pocket_spib/r2_S100B_b1/S100B_b1_input/colvar_0_unb.npy,/scratch/zt1/project/tiwary-prj/user/xg23/S100/rave/pocket_spib/r2_S100B_b1/S100B_b1_input/colvar_1_unb.npy,/scratch/zt1/project/tiwary-prj/user/xg23/S100/rave/pocket_spib/r2_S100B_b1/S100B_b1_input/colvar_2_unb.npy]

 initial_labels = [/scratch/zt1/project/tiwary-prj/user/xg23/S100/rave/pocket_spib/r2_S100B_b1/S100B_b1_input/labels_0_unb.npy,/scratch/zt1/project/tiwary-prj/user/xg23/S100/rave/pocket_spib/r2_S100B_b1/S100B_b1_input/labels_1_unb.npy,/scratch/zt1/project/tiwary-prj/user/xg23/S100/rave/pocket_spib/r2_S100B_b1/S100B_b1_input/labels_2_unb.npy]

 traj_weights 
